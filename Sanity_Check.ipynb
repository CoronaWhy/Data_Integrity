{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khachatur\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, json\n",
    "\n",
    "from os import listdir\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data'\n",
    "VERSION = 'cord19_v20'\n",
    "META_PATH =\"/\".join([DATAPATH, VERSION, 'metadata.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63571, 18)\n"
     ]
    }
   ],
   "source": [
    "meta_df = pd.read_csv(META_PATH)\n",
    "print(meta_df.shape)\n",
    "# meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cuid duplicate records: 44\n"
     ]
    }
   ],
   "source": [
    "cuid_dup = meta_df[meta_df.duplicated(['cord_uid'])]\n",
    "cuid_dup_set = set(cuid_dup['cord_uid'])\n",
    "print(\"All cuid duplicate records:\", len(cuid_dup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in cuid_dup_set:\n",
    "    dup_df = meta_df[meta_df['cord_uid'] == uid]\n",
    "    if len(dup_df['pubmed_id'].unique()) != 1 and len(dup_df['doi'].unique()) != 1:\n",
    "        print(dup_df['pubmed_id'].unique())\n",
    "        print(dup_df['doi'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate pdf paths: 13\n",
      "Total pdf paths 48924\n"
     ]
    }
   ],
   "source": [
    "has_pdfpar_df = meta_df[meta_df['pdf_json_files'].isna() == False]\n",
    "pdfpar_dup = has_pdfpar_df[has_pdfpar_df.duplicated(['pdf_json_files'])]\n",
    "print(\"Duplicate pdf paths:\", len(pdfpar_dup))\n",
    "print(\"Total pdf paths\", len(has_pdfpar_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicte pmc paths: 0\n",
      "Total pmc paths 33503\n"
     ]
    }
   ],
   "source": [
    "has_pmcpar_df = meta_df[meta_df['pmc_json_files'].isna() == False]\n",
    "pmcpar_dup = has_pmcpar_df[has_pmcpar_df.duplicated(['pmc_json_files'])]\n",
    "print(\"Duplicte pmc paths:\", len(pmcpar_dup))\n",
    "print(\"Total pmc paths\", len(has_pmcpar_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records with multiple associated sha values: 2517\n"
     ]
    }
   ],
   "source": [
    "has_sha_df = meta_df[meta_df['sha'].isna() == False]\n",
    "mult_sha_df = has_sha_df[has_sha_df['sha'].str.len() != 40]\n",
    "sing_sha_df = has_sha_df[has_sha_df['sha'].str.len() == 40]\n",
    "print(\"# of records with multiple associated sha values:\", len(mult_sha_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of weird pmcid values: 0\n"
     ]
    }
   ],
   "source": [
    "has_pmc_df = meta_df[meta_df['pmcid'].isna() == False]\n",
    "print(\"# of weird pmcid values:\", len(has_pmc_df[(has_pmc_df['pmcid'].str.len() != 9) & (has_pmc_df['pmcid'].str.len() != 10) & (has_pmc_df['pmcid'].str.len() != 8)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSE_PATH =\"/\".join([DATAPATH, VERSION, 'document_parses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # pdfs: 33503\n",
      "Total # pmcs: 51868\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"/\".join([PARSE_PATH, 'pdf_json'])\n",
    "pmc_path = \"/\".join([PARSE_PATH, 'pmc_json'])\n",
    "pdf_parses = listdir(pdf_path)\n",
    "pmc_parses = listdir(pmc_path)\n",
    "print(\"Total # pdfs:\", len(pmc_parses))\n",
    "print(\"Total # pmcs:\", len(pdf_parses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sha values in multi sha record list: 5475\n",
      "All UNIQUE sha values in multi sha record list: 5475\n",
      "All sha values in single sha record list: 46407\n",
      "All UNIQUE sha values in single sha record list: 46394\n",
      "All sha values in combined sha record list: 51882\n",
      "All UNIQUE sha values in combined sha record list: 51868\n"
     ]
    }
   ],
   "source": [
    "sha_list = []\n",
    "for row in mult_sha_df.iterrows():\n",
    "    new_l = row[1].sha.split(\"; \")\n",
    "    sha_list.extend(new_l)\n",
    "\n",
    "multi_sha_list = sha_list.copy()\n",
    "print(\"All sha values in multi sha record list:\", len(multi_sha_list))\n",
    "print(\"All UNIQUE sha values in multi sha record list:\", len(set(multi_sha_list)))\n",
    "\n",
    "print(\"All sha values in single sha record list:\", len(list(sing_sha_df.sha)))\n",
    "print(\"All UNIQUE sha values in single sha record list:\", len(set(sing_sha_df.sha)))\n",
    "\n",
    "sha_list.extend(list(sing_sha_df.sha))\n",
    "print(\"All sha values in combined sha record list:\", len(sha_list))\n",
    "print(\"All UNIQUE sha values in combined sha record list:\", len(set(sha_list)))\n",
    "total_unique_sha = len(set(sha_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total full text records: 49862\n"
     ]
    }
   ],
   "source": [
    "has_path_df = meta_df[(meta_df[\"pdf_json_files\"].isna() == False) | (meta_df[\"pmc_json_files\"].isna() == False)]\n",
    "print(\"Total full text records:\", len(has_path_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_ratios(data, record, field):\n",
    "    if field == 'title':\n",
    "        json_text = data['metadata'][field]\n",
    "    else:\n",
    "        if field not in data.keys():\n",
    "            json_text = ''\n",
    "        else:\n",
    "            json_text = ''\n",
    "            for sub_dict in data[field]:\n",
    "                json_text  = json_text + sub_dict['text'] + ' '\n",
    "    \n",
    "    json_text = json_text.lower()\n",
    "        \n",
    "    try:\n",
    "        meta_text = record[field].lower()\n",
    "    except:\n",
    "        meta_text = ''\n",
    "    try:    \n",
    "        full_ratio = fuzz.ratio(meta_text, json_text)\n",
    "        partial_ratio = fuzz.partial_ratio(meta_text, json_text)\n",
    "    except:\n",
    "        print('META:   ', meta_text)\n",
    "        print('JSON:   ', json_text)\n",
    "        print('P_ID:   ', data['paper_id'] )\n",
    "        \n",
    "    return json_text, meta_text, full_ratio, partial_ratio\n",
    "\n",
    "def set_text_and_ratios(sub_dict, field, json_text, meta_text, full_ratio, partial_ratio):\n",
    "    sub_dict[field]['meta_text'] = meta_text\n",
    "    sub_dict[field]['json_text'] = json_text\n",
    "    sub_dict[field]['full_ratio'] = full_ratio\n",
    "    sub_dict[field]['partial_ratio'] = partial_ratio\n",
    "    \n",
    "    if full_ratio >= 85 or partial_ratio >= 85:\n",
    "        sub_dict[field]['is_error'] = False\n",
    "    else:\n",
    "        sub_dict[field]['is_error'] = True\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49849\n"
     ]
    }
   ],
   "source": [
    "errors_dict = dict()\n",
    "for row in has_path_df.iterrows():\n",
    "    record = row[1]\n",
    "    if record.cord_uid in errors_dict.keys():\n",
    "        continue\n",
    "    else:\n",
    "        errors_dict[record.cord_uid] = {}\n",
    "    \n",
    "    uid_dict = errors_dict[record.cord_uid]\n",
    "    \n",
    "    pmc_path = None\n",
    "    pmcid = None\n",
    "    pdf_paths = None\n",
    "    pdf_ids = None\n",
    "    data = None\n",
    "    \n",
    "    if not isinstance(record.pmc_json_files, float):\n",
    "        pmc_path = \"/\".join([DATAPATH, VERSION, record.pmc_json_files])\n",
    "        pmcid = record.pmcid\n",
    "    \n",
    "    if not isinstance(record.pdf_json_files, float):\n",
    "        pdf_paths = record.pdf_json_files.split(\"; \")\n",
    "        pdf_paths = [\"/\".join([DATAPATH, VERSION, x]) for x in pdf_paths]\n",
    "        pdf_ids = record.sha.split(\"; \")\n",
    "    \n",
    "    if pmcid is not None:\n",
    "        uid_dict[pmcid] = {'title': {}}\n",
    "        with open(pmc_path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        json_text, meta_text, full_ratio, partial_ratio = get_text_and_ratios(data, record, 'title')\n",
    "        set_text_and_ratios(uid_dict[pmcid], 'title', json_text, meta_text, full_ratio, partial_ratio)\n",
    "    \n",
    "    if pdf_ids is not None:\n",
    "        for i, path in enumerate(pdf_paths):\n",
    "            \n",
    "            uid_dict[pdf_ids[i]] = {'title': {}, 'abstract':{}}\n",
    "            with open(path) as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            json_text, meta_text, full_ratio, partial_ratio = get_text_and_ratios(data, record, 'title')\n",
    "            set_text_and_ratios(uid_dict[pdf_ids[i]], 'title', json_text, meta_text, full_ratio, partial_ratio)\n",
    "            \n",
    "            json_text, meta_text, full_ratio, partial_ratio = get_text_and_ratios(data, record, 'abstract')\n",
    "            set_text_and_ratios(uid_dict[pdf_ids[i]], 'abstract', json_text, meta_text, full_ratio, partial_ratio)\n",
    "            \n",
    "print(len(errors_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in meta_df[meta_df['sha'] == '4b0e97f3c2c4402b3174623db30ce1a1cde46b50'].iterrows():\n",
    "    meta_text = record[1]['abstract'].lower()\n",
    "    \n",
    "    path = \"/\".join([DATAPATH, VERSION, record[1]['pdf_json_files']])\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    json_text = ''\n",
    "    for sub_dict in data['abstract']:\n",
    "        json_text  = json_text + sub_dict['text'] + ' '\n",
    "    \n",
    "    json_text = json_text.lower()\n",
    "    \n",
    "    full_ratio = fuzz.ratio(meta_text, json_text)\n",
    "    partial_ratio = fuzz.partial_ratio(json_text, meta_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(errors_dict, main_dict, pid):\n",
    "    if main_dict['is_error']:\n",
    "        errors_dict['total_errors'] += 1\n",
    "        if main_dict['meta_text'] == '':\n",
    "            errors_dict['empty_meta_text'] += 1\n",
    "        if main_dict['json_text'] == '':\n",
    "            errors_dict['empty_json_text'] += 1\n",
    "        if errors_dict['empty_meta_text'] != '' and main_dict['json_text'] !='':\n",
    "            errors_dict['other_errors'] += 1\n",
    "        if len(pid) == 40:\n",
    "            errors_dict['sha_errors'] += 1\n",
    "        if len(pid) != 40:\n",
    "            errors_dict['pmc_errors'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_errors': 9066, 'empty_json_text': 5911, 'empty_meta_text': 6, 'other_errors': 3155, 'sha_errors': 9045, 'pmc_errors': 21}\n",
      "{'total_errors': 13602, 'empty_json_text': 8952, 'empty_meta_text': 1043, 'other_errors': 4650, 'sha_errors': 13602, 'pmc_errors': 0}\n"
     ]
    }
   ],
   "source": [
    "title_errors_dict = {\"total_errors\": 0, \"empty_json_text\": 0, \"empty_meta_text\": 0,\n",
    "                     \"other_errors\": 0, \"sha_errors\": 0, \"pmc_errors\": 0}\n",
    "\n",
    "abstract_errors_dict = {\"total_errors\": 0, \"empty_json_text\": 0, \"empty_meta_text\": 0,\n",
    "                     \"other_errors\": 0, \"sha_errors\": 0, \"pmc_errors\": 0}\n",
    "\n",
    "for cord_uid in errors_dict:\n",
    "    for pid in errors_dict[cord_uid]:\n",
    "        title_dict = errors_dict[cord_uid][pid]['title']\n",
    "        get_errors(title_errors_dict, title_dict, pid)\n",
    "        \n",
    "        if len(pid) == 40:\n",
    "            abstract_dict = errors_dict[cord_uid][pid]['abstract']\n",
    "            get_errors(abstract_errors_dict, abstract_dict, pid)\n",
    "        \n",
    "print(title_errors_dict)\n",
    "print(abstract_errors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('qc_results'):\n",
    "    os.mkdir('qc_results')\n",
    "if not os.path.isdir('qc_results/' + VERSION):\n",
    "    os.mkdir('qc_results/' + VERSION)\n",
    "\n",
    "RESULTS_PATH = 'qc_results/' + VERSION + '/'\n",
    "\n",
    "all_titles_and_abstracts_path = RESULTS_PATH + 'all_titles_and_abstracts.json'\n",
    "with open(all_titles_and_abstracts_path, 'w') as fp:\n",
    "    json.dump(errors_dict, fp)\n",
    "\n",
    "title_errors_info_path = RESULTS_PATH + 'title_errors_info.json'\n",
    "with open(title_errors_info_path, 'w') as fp:\n",
    "    json.dump(title_errors_dict, fp)\n",
    "    \n",
    "abstract_errors_info_path = RESULTS_PATH + 'abstract_errors_info.json'\n",
    "with open(abstract_errors_info_path, 'w') as fp:\n",
    "    json.dump(abstract_errors_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = RESULTS_PATH + \"quality_check_results_log.txt\"\n",
    "f = open(log_path, \"w\")\n",
    "f.write(\"Total records in metadata: {}\\n\".format(meta_df.shape[0]))\n",
    "f.write(\"Total full text records: {}\\n\".format(len(has_path_df)))\n",
    "f.write(\"All cuid duplicate records: {}\\n\".format(len(cuid_dup)))\n",
    "\n",
    "f.write(\"\\nTotal pdf paths: {}\\n\".format(len(has_pdfpar_df)))\n",
    "f.write(\"Duplicte pdf paths: {}\\n\".format(len(pdfpar_dup)))\n",
    "f.write(\"Total pmc paths: {}\\n\".format(len(has_pmcpar_df)))\n",
    "f.write(\"Duplicte pmc paths: {}\\n\".format(len(pmcpar_dup)))\n",
    "\n",
    "f.write(\"\\nRecords with multiple sha values: {}\\n\".format(len(mult_sha_df)))\n",
    "f.write(\"Record with weird pmcid values: {}\\n\".format(len(has_pmc_df[(has_pmc_df['pmcid'].str.len() != 9) & (has_pmc_df['pmcid'].str.len() != 10) & (has_pmc_df['pmcid'].str.len() != 8)])))\n",
    "\n",
    "f.write(\"\\nTotal # of pdfs: {}\\n\".format(len(pmc_parses)))\n",
    "f.write(\"Total # of pmcs: {}\\n\".format(len(pdf_parses)))\n",
    "\n",
    "f.write(\"\\nAll sha values in multi sha record list: {}\\n\".format(len(multi_sha_list)))\n",
    "f.write(\"All UNIQUE sha values in multi sha record list: {}\\n\".format(len(set(multi_sha_list))))\n",
    "f.write(\"All sha values in single sha record list: {}\\n\".format(len(list(sing_sha_df.sha))))\n",
    "f.write(\"All UNIQUE sha values in single sha record list: {}\\n\".format(len(set(sing_sha_df.sha))))\n",
    "f.write(\"All sha values in combined sha record list: {}\\n\".format(len(sha_list)))\n",
    "f.write(\"All UNIQUE sha values in combined sha record list: {}\\n\".format(len(set(sha_list))))\n",
    "\n",
    "f.write(\"\\nAll title discrepencies between metadata and jsons: {}\\n\".format(title_errors_dict['total_errors']))\n",
    "f.write(\"All empty title text fields in jsons: {}\\n\".format(title_errors_dict['empty_json_text']))\n",
    "f.write(\"All empty title text fields in metadata: {}\\n\".format(title_errors_dict['empty_meta_text']))\n",
    "f.write(\"All other title text field discrepencies: {}\\n\".format(title_errors_dict['other_errors']))\n",
    "f.write(\"All title errors that occured with pdfs: {}\\n\".format(title_errors_dict['sha_errors']))\n",
    "f.write(\"All title errors that occured with pmcs: {}\\n\".format(title_errors_dict['pmc_errors']))\n",
    "\n",
    "f.write(\"\\nAll abstract discrepencies between metadata and jsons: {}\\n\".format(abstract_errors_dict['total_errors']))\n",
    "f.write(\"All empty abstract text fields in jsons: {}\\n\".format(abstract_errors_dict['empty_json_text']))\n",
    "f.write(\"All empty absract text fields in metadata: {}\\n\".format(abstract_errors_dict['empty_meta_text']))\n",
    "f.write(\"All other abstract text field discrepencies: {}\\n\".format(abstract_errors_dict['other_errors']))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
